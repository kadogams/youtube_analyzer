{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A class to make a sentiment and emotion analysis on Youtube comments.\n",
    "The analysis are saved locally in an SQLite database.\n",
    "Database schema available at 'data/sqlite_schema/sqlite_diagram.png'\n",
    "\n",
    "requirements:\n",
    " - google-api-python-client\n",
    " - ibm-watson\n",
    " - pandas\n",
    " - tqdm\n",
    "\n",
    "API keys required for the following methods:\n",
    " - self.search(): Google API\n",
    " - self.run_analysis(): Microsoft Azure Text Analytics API\n",
    "                        IBM Watson Natural Language Understanding API\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from time import sleep\n",
    "\n",
    "# from langdetect import detect\n",
    "import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "from google.cloud import translate\n",
    "\n",
    "from sqlite3_wrapper.database import Database\n",
    "from azure_api import get_key_phrases, get_languages, get_sentiments\n",
    "from watson_api import get_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQLite\n",
    "CONFLIT_RESOLUTION_ALGORITHMS = ['ROLLBACK', 'ABORT', 'FAIL', 'IGNORE', 'REPLACE']\n",
    "\n",
    "# Google API Client\n",
    "MAX_SEARCH = 50\n",
    "MAX_COMMENT_THREADS = 100\n",
    "RESULT_ORDERS = ['date', 'rating', 'relevance', 'title', 'videoCount', 'viewCount']\n",
    "COMMENT_ORDERS = ['time', 'relevance']\n",
    "\n",
    "# MS Azure\n",
    "AZURE_MAX_DOCUMENTS = 1000\n",
    "# supported languages for both sentiment analysis and key phrases extraction:\n",
    "AZURE_SUPPORTED_LANG = ['da', 'nl', 'en', 'fi', 'fr', 'de', 'it', 'no', 'pl', 'pt', 'ru', 'es', 'sv']\n",
    "\n",
    "# IBM Watson (supported languages for emotion analysis)\n",
    "WATSON_SUPPORTED_LANG = ['en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class youtubeAnalyzer(Database):\n",
    "    \"\"\" A class to manage the YouTube SQLite database.\n",
    "    Inherits from base class 'Database' in database.py (a wrapper around the sqlite3 python library)\n",
    "    Database schema available at 'data/sqlite_schema/sqlite_diagram.png'\n",
    "    \"\"\"\n",
    "\n",
    "    #################\n",
    "    # Magic Methods #\n",
    "    #################\n",
    "\n",
    "    def __init__(self, googleApiKey=None, azureApiKey=None, watsonApiKey=None,\n",
    "                 azureBaseUrl='https://westcentralus.api.cognitive.microsoft.com/text/analytics/v2.1/',\n",
    "                 watsonBaseUrl='https://gateway-lon.watsonplatform.net/natural-language-understanding/api',\n",
    "                 conflict_resolution='IGNORE', sqlite_file='youtube.sqlite'):\n",
    "        \"\"\"\n",
    "        :param str googleApiKey: Developper key for Google API\n",
    "        :param str azureApiKey: Subscription key for Azure Text Analytics API\n",
    "        :param str watsonApiKey: API key for IBM Watson's Natural Language Understanding API\n",
    "        :param str azureBaseUrl: Base url for Azure Text Analytics API\n",
    "        :param str watsonBaseUrl_nlu: Base url for IBM Watson's Natural Language Understanding API\n",
    "        :param str conflict_resolution: ON CONFLICT clause for the SQLite queries. Warning: 'REPLACE' will delete the all Azure and Watson analysis.\n",
    "        :param str sqlite_file: SQLite file name\n",
    "        \"\"\"\n",
    "        if conflict_resolution not in CONFLIT_RESOLUTION_ALGORITHMS:\n",
    "            raise ValueError(\"Valid values for the `conflict_resolution` param are '{}', \"\n",
    "                             \"the given value is invalid: '{}'\"\n",
    "                             .format(\"', '\".join(CONFLIT_RESOLUTION_ALGORITHMS), conflict_resolution))\n",
    "\n",
    "#         path = os.path.dirname(os.path.abspath(__file__)) + '/data'\n",
    "        path = os.getcwd() + '/data'\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        self.dir = path + '/' + sqlite_file\n",
    "        self.conn = None\n",
    "        self.cursor = None\n",
    "        self.conflict_resolution = conflict_resolution\n",
    "        \n",
    "        self.googleApiKey = googleApiKey\n",
    "        self.azureApiKey = azureApiKey\n",
    "        self.azureBaseUrl = azureBaseUrl\n",
    "        self.watsonApiKey = watsonApiKey\n",
    "        self.watsonBaseUrl = watsonBaseUrl\n",
    "\n",
    "        try:\n",
    "            self._init_youtube()\n",
    "        except Exception as e:\n",
    "            print('Could not connect to the Google API Client:', e)\n",
    "        \n",
    "        Database.__init__(self, name=self.dir) # init self.conn and self.cursor\n",
    "        if self.conn is not None:\n",
    "            print('***** YouTube database directory: {} *****'.format(self.dir))\n",
    "    \n",
    "   \n",
    "    \n",
    "    ##################\n",
    "    # Public Methods #\n",
    "    ##################\n",
    "    \n",
    "    def create_structure(self):\n",
    "        \"\"\" Create the wap SQLite database structure (cf. data/sqlite_schema/sqlite_diagram.png).\n",
    "        SQL 'CREATE TABLE' statements available in 'data/sqlite_schema/*_schema.txt'\n",
    "        \"\"\"\n",
    "\n",
    "        def _create_table(create_table_sql):\n",
    "            \"\"\" Create a table from the create_table_sql statement\n",
    "            \n",
    "            :param str create_table_sql: a CREATE TABLE statement\n",
    "            \"\"\"\n",
    "            try:\n",
    "                self.query(create_table_sql)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "        path = os.path.dirname(self.dir) + '/sqlite_schema'\n",
    "        with open(path + '/channels_schema.txt','r') as f:\n",
    "            sql_create_channels_table = f.read()\n",
    "        with open(path + '/comments_schema.txt','r') as f:\n",
    "            sql_create_comments_table = f.read()\n",
    "        with open(path + '/videos_schema.txt','r') as f:\n",
    "            sql_create_videos_table = f.read()\n",
    "        if self.conn is not None:\n",
    "            _create_table(sql_create_channels_table)\n",
    "            _create_table(sql_create_comments_table)\n",
    "            _create_table(sql_create_videos_table)\n",
    "            self.conn.commit()\n",
    "        else:\n",
    "            print(\"Error: Cannot create the database connection.\")\n",
    "    \n",
    "    \n",
    "    def display_schema(self):\n",
    "        \"\"\" Print the database schemas\n",
    "        \"\"\"\n",
    "        if not self.conn:\n",
    "            return\n",
    "        sql = \"SELECT SQL FROM sqlite_master WHERE TYPE = 'table'\"\n",
    "        self.query(sql)\n",
    "        tables = self.cursor.fetchall()\n",
    "        for table in tables:\n",
    "            print(*table, '\\n')\n",
    "    \n",
    "    \n",
    "    def get_comments_df(self, video_search=None, video_separator='OR',\n",
    "                        channel_search=None, channel_separator='OR',\n",
    "                        from_date=None, to_date=None):\n",
    "        \"\"\" Return a DataFrame of comments to the contents with the specified criterias\n",
    "        in the SQLite database\n",
    "\n",
    "        :param str video_search: Comma separated words for a keyword search in videos.title and videos.description\n",
    "        :param str video_separator: Choose between 'AND' (match every words) or 'OR' (match any word) for the `video_search` param\n",
    "        :param str channel_search: Comma separated words for a keyword search in channels.title and channels.description\n",
    "        :param str channel_separator: Choose between 'AND' (match every words) or 'OR' (match any word) for the `channel_search` param\n",
    "        :param datetime-like from_date: From specified comment published date\n",
    "        :param datetime-like to_date: To specified comment published date\n",
    "        \"\"\"\n",
    "        for separator in (video_separator, channel_separator):\n",
    "            if separator != 'AND' and separator !='OR':\n",
    "                raise ValueError(\"Valid values for the `_separator` params are 'AND' or 'OR', \"\n",
    "                                 \"the given value is invalid: '{}'\".format(separator))\n",
    "\n",
    "        condition_list = []\n",
    "        condition_list.append(self._format_datetime_condition(from_date, to_date))\n",
    "        condition_list.append(self._format_search_condition(video_search, video_separator,\n",
    "                                                            channel_search, channel_separator))\n",
    "        condition_list = list(filter(None, condition_list))\n",
    "        if condition_list:\n",
    "            conditions = 'WHERE' + ' AND '.join(condition_list)\n",
    "        else:\n",
    "            conditions = ''\n",
    "\n",
    "        sql = f\"\"\"\n",
    "            SELECT DISTINCT\n",
    "                comments.*,\n",
    "                channels.country\n",
    "            FROM\n",
    "                comments\n",
    "            INNER JOIN\n",
    "                videos ON videos.id = comments.videoId\n",
    "            INNER JOIN\n",
    "                channels ON channels.id = comments.authorChannelId\n",
    "            {conditions}\n",
    "        \"\"\"\n",
    "        df = pd.read_sql_query(sql, self.conn)\n",
    "        df['publishedAt'] = pd.to_datetime(df['publishedAt'])\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def run_analysis(self):\n",
    "        \"\"\" Run a sentiment analysis on the comments of the SQLite database\n",
    "        via MS Azure Text Analytics and an emotion analysis via\n",
    "        IBM Watson NLU if their detected language is supported by the APIs.\n",
    "        The analysis is stored in the SQLite database.\n",
    "        \"\"\"\n",
    "        self._update_languages()\n",
    "#         self._update_sentiments()\n",
    "#         self._update_keywords()\n",
    "#         self._update_emotions()\n",
    "        self.conn.commit()\n",
    "        print('Analysis complete.')\n",
    "    \n",
    "    \n",
    "    def search(self, query, n_results=20, result_order='relevance',\n",
    "               n_comments=100, comment_order='relevance', include_replies=False):\n",
    "        \"\"\" Search specified videos on YouTube, and update the local database accordingly\n",
    "        with a sentiment and emotion analysis on the associated comments.\n",
    "        \n",
    "        For more information about the search options, please refer\n",
    "        to the documentation at:\n",
    "        https://developers.google.com/youtube/v3/docs/search/list\n",
    "        \n",
    "        :param str query: Query term to search for\n",
    "        :param int n_results: Number of search results desired\n",
    "        :param str result_order: Order of the search results in the API response\n",
    "        :param int n_comments: Number of comment threads per video desired\n",
    "        :param str comment_order: Order of the comment threads in the API response\n",
    "        :param bool include_replies: Include replies to the comments, if any\n",
    "        \"\"\"\n",
    "        if self.youtube == None:\n",
    "            try:\n",
    "                self._init_youtube()\n",
    "            except Exception as e:\n",
    "                print('Could not connect to the Google API Client:', e)\n",
    "        \n",
    "        if result_order not in RESULT_ORDERS:\n",
    "            raise ValueError(\"Valid values for the `result_order` param are '{}', \"\n",
    "                             \"the given value is invalid: '{}'\"\n",
    "                             .format(\"', '\".join(RESULT_ORDERS), result_order))\n",
    "\n",
    "        if comment_order not in COMMENT_ORDERS:\n",
    "            raise ValueError(\"Valid values for the `result_order` param are '{}', \"\n",
    "                             \"the given value is invalid: '{}'\"\n",
    "                             .format(\"', '\".join(COMMENT_ORDERS), comment_order))\n",
    "        \n",
    "        response_list = []\n",
    "        page_token = None\n",
    "        for n in range(0, n_results, MAX_SEARCH):\n",
    "            # Maximum number of search results per page: 50\n",
    "            max_results = MAX_SEARCH if n_results - n > MAX_SEARCH else n_results - n\n",
    "            try:\n",
    "                search_response = self.youtube.search().list(\n",
    "                    part='snippet',\n",
    "                    maxResults=max_results,\n",
    "                    order=result_order, # You may consider using 'viewCount'\n",
    "                    pageToken=page_token,\n",
    "                    q=query,\n",
    "                    safeSearch='none',\n",
    "                    type='video', # Channels might appear in search results\n",
    "                ).execute()\n",
    "            except Exception as e:\n",
    "                print(f\"An error occured during a search request on YouTube API:\", e)\n",
    "                continue\n",
    "            response_list.append(search_response)\n",
    "            \n",
    "            if not 'nextPageToken' in search_response:\n",
    "                break\n",
    "            page_token = search_response['nextPageToken']\n",
    "        \n",
    "        videoId_list = self._insert_videos(response_list, n_results)\n",
    "        self._get_comments(videoId_list, n_comments, comment_order, include_replies)\n",
    "        self.conn.commit()\n",
    "        print(f\"Search results stored in '{self.dir}'\")\n",
    "    \n",
    "    \n",
    "    ###################\n",
    "    # Private Methods #\n",
    "    ###################\n",
    "        \n",
    "    def _format_comment_resource(self, comment_resource):\n",
    "        \"\"\" Format the comment resource into a list of tuples\n",
    "        for the SQLite query.\n",
    "        \n",
    "        :param dict comment_resource: Information about a single YouTube comment\n",
    "        :return: Specific values of the comment resource\n",
    "        :rtype: list\n",
    "        \"\"\"\n",
    "        if 'authorChannelId' in comment_resource['snippet'] \\\n",
    "          and 'value' in comment_resource['snippet']['authorChannelId']:\n",
    "            authorChannelId = comment_resource['snippet']['authorChannelId']['value']\n",
    "            self._insert_channel(authorChannelId)\n",
    "        else:\n",
    "            authorChannelId = None\n",
    "        \n",
    "        values = (\n",
    "            comment_resource['id'],\n",
    "            comment_resource['snippet']['videoId'],\n",
    "            authorChannelId,\n",
    "            str(pd.to_datetime(comment_resource['snippet']['publishedAt'])),\n",
    "            comment_resource['snippet']['likeCount'],\n",
    "            comment_resource['snippet']['parentId'] if 'parentId' in comment_resource['snippet'] else None,\n",
    "            comment_resource['snippet']['textDisplay']\n",
    "        )\n",
    "        return values\n",
    "    \n",
    "    \n",
    "    def _format_datetime_condition(self, from_date, to_date):\n",
    "        \"\"\" Return a condition for the SQLite 'WHERE' clause with the specified datetime range\n",
    "\n",
    "        :param datetime-like from_date: From specified comment published date\n",
    "        :param datetime-like to_date: To specified comment published date\n",
    "        :return: A condition for the SQLite 'WHERE' clause\n",
    "        :rtype: str\n",
    "        \"\"\"\n",
    "        if not from_date and not to_date:\n",
    "            return None\n",
    "        elif not from_date:\n",
    "            from_date = '1900-01-01 00:00:00'\n",
    "        elif not to_date:\n",
    "            to_date = 'now'\n",
    "        return \"(contents.published BETWEEN '{}' AND '{}')\".format(str(pd.to_datetime(from_date)),\n",
    "                                                                   str(pd.to_datetime(to_date)))\n",
    "    \n",
    "\n",
    "    def _format_search_condition(self, video_search, video_separator, channel_search, channel_separator):\n",
    "        \"\"\" Return a condition for the SQLite 'WHERE' clause with the specified search query.\n",
    "\n",
    "        :param str video_search: Comma separated words for a keyword search in videos.title and videos.description\n",
    "        :param str video_separator: Choose between 'AND' (match every words) or 'OR' (match any word) for the `video_search` param\n",
    "        :param str channel_search: Comma separated words for a keyword search in channels.title and channels.description\n",
    "        :param str channel_separator: Choose between 'AND' (match every words) or 'OR' (match any word) for the `channel_search` param\n",
    "        :return: A condition for the SQLite 'WHERE' clause\n",
    "        :rtype: str\n",
    "        \"\"\"\n",
    "        if not video_search and not channel_search:\n",
    "            return None\n",
    "        conditions = []\n",
    "        if video_search:\n",
    "            word_list = [\"'%{}%'\".format(word.strip()) for word in video_search.split(',') if word.strip()]\n",
    "            video_conditions = [f'videos.title LIKE {word} OR videos.description LIKE {word}'\n",
    "                                for word in word_list]\n",
    "            if video_conditions:\n",
    "                separator = f' {video_separator} '\n",
    "                conditions.append('({})'.format(separator.join(video_conditions)))\n",
    "        if channel_search:\n",
    "            word_list = [\"'%{}%'\".format(word.strip()) for word in channel_search.split(',') if word.strip()]\n",
    "            channel_conditions = [f'channels.title LIKE {word} OR channels.description LIKE {word}'\n",
    "                                  for word in word_list]\n",
    "            if channel_conditions:\n",
    "                separator = f' {video_separator} '\n",
    "                conditions.append('({})'.format(separator.join(channel_conditions)))\n",
    "        if len(conditions) == 1:\n",
    "            return conditions.pop()\n",
    "        return '({})'.format(' OR '.join(conditions))\n",
    "\n",
    "    \n",
    "    def _get_comments(self, videoId_list, n_comments, comment_order, include_replies):\n",
    "        \"\"\" Get the comment threads to the videos found with the search request.\n",
    "        \n",
    "        :paran list videoId_list: Ids of the comments' parent video\n",
    "        :param int n_comments: Number of comment threads per video desired\n",
    "        :param str order: Order of the resources in the API response\n",
    "        :param bool include_replies: Include replies to the comments, if any\n",
    "        \"\"\"\n",
    "        if include_replies == False:\n",
    "            part = 'snippet'\n",
    "        else:\n",
    "            part = 'snippet,replies'\n",
    "        \n",
    "        progress_bar = tqdm(videoId_list)\n",
    "        progress_bar.set_description('Fetching comment threads by video')\n",
    "        for videoId in progress_bar:\n",
    "            \n",
    "            response_list = []\n",
    "            page_token = None\n",
    "            for n in range(0, n_comments, MAX_COMMENT_THREADS):\n",
    "                # Maximum number of comment threads per page: 100\n",
    "                max_comments = MAX_COMMENT_THREADS if n_comments - n > MAX_COMMENT_THREADS else n_comments - n\n",
    "                try:\n",
    "                    comment_response = self.youtube.commentThreads().list(\n",
    "                        part=part,\n",
    "                        maxResults=max_comments,\n",
    "                        order=comment_order,\n",
    "                        pageToken=page_token,\n",
    "                        textFormat = 'plainText',\n",
    "                        videoId=videoId,\n",
    "                    ).execute()\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occured during the commmentThreads request of videoId '{videoId}':\", e)\n",
    "                    continue\n",
    "                response_list.append(comment_response)\n",
    "                \n",
    "                if not 'nextPageToken' in comment_response:\n",
    "                    break\n",
    "                page_token = comment_response['nextPageToken']\n",
    "            \n",
    "            self._insert_comments(response_list, n_comments)\n",
    "\n",
    "    \n",
    "    def _init_youtube(self):\n",
    "        # Disable OAuthlib's HTTPS verification when running locally.\n",
    "        # *DO NOT* leave this option enabled in production.\n",
    "        os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n",
    "        \n",
    "        api_service_name = \"youtube\"\n",
    "        api_version = \"v3\"\n",
    "        self.youtube = build(\n",
    "            api_service_name,\n",
    "            api_version,\n",
    "            developerKey=self.googleApiKey\n",
    "        )\n",
    "    \n",
    "\n",
    "    def _insert_channel(self, channelId):\n",
    "        \"\"\" Insert the specified channel's info into the SQLite 'channels' table.\n",
    "\n",
    "        :param str channelId: Id of the channel\n",
    "        \"\"\"\n",
    "        try:\n",
    "            channel_response = self.youtube.channels().list(\n",
    "                part='snippet',\n",
    "                id=channelId\n",
    "            ).execute()\n",
    "        except Exception as e:\n",
    "            print(f\"An error occured during the channels request of channelId '{channelId}':\", e)\n",
    "            return\n",
    "        \n",
    "        if not 'items' in channel_response:\n",
    "            return\n",
    "        # The response should contain one item.\n",
    "        for channel_resource in channel_response['items']:\n",
    "            values = (\n",
    "                channel_resource['id'],\n",
    "                channel_resource['snippet']['title'],\n",
    "                channel_resource['snippet']['description'],\n",
    "                channel_resource['snippet']['country'] if 'country' in channel_resource['snippet'] else None\n",
    "            )\n",
    "            sql = f'INSERT OR {self.conflict_resolution} INTO channels VALUES(?,?,?,?)'\n",
    "            if not values:\n",
    "                return\n",
    "            self.cursor.execute(sql, values)\n",
    "    \n",
    "    \n",
    "    def _insert_comments(self, response_list, n_comments):\n",
    "        \"\"\" Insert the collection of comment threads into the SQLite 'comments' table.\n",
    "        \n",
    "        :param list response_list: Responses to the commentThreads request\n",
    "        :param int n_comments: Number of comment threads per video desired\n",
    "        \"\"\"\n",
    "        value_list = []\n",
    "        progress_bar = tqdm(total=n_comments)\n",
    "        progress_bar.set_description('Processing comments')\n",
    "        i = 0\n",
    "        for comment_response in response_list:\n",
    "            if not 'items' in comment_response:\n",
    "                continue\n",
    "\n",
    "            for item in comment_response['items']:\n",
    "                values = self._format_comment_resource(item['snippet']['topLevelComment'])\n",
    "                value_list.append(values)\n",
    "                if 'replies' in item:\n",
    "                    for comment in item['replies']['comments']:\n",
    "                        values = self._format_comment_resource(comment)\n",
    "                        value_list.append(values)\n",
    "                progress_bar.update(1)\n",
    "                i+=1\n",
    "                print(i)\n",
    "\n",
    "        cols = 'id,videoId,authorChannelId,publishedAt,likeCount,parentId,text'\n",
    "        sql = f'INSERT OR {self.conflict_resolution} INTO comments({cols}) VALUES(?,?,?,?,?,?,?)'\n",
    "        if value_list:\n",
    "            self.cursor.executemany(sql, value_list)\n",
    "\n",
    "            \n",
    "    \n",
    "    def _insert_videos(self, response_list, n_results):\n",
    "        \"\"\" Insert the collection of search results into the SQLite 'videos' table.\n",
    "        \n",
    "        :param list response_list: Responses to the search request\n",
    "        :param int n_results: Number of search results desired\n",
    "        :return: Ids of the videos inserted\n",
    "        :rtype: list\n",
    "        \"\"\"\n",
    "        value_list = []\n",
    "        videoId_list = []\n",
    "        progress_bar = tqdm(total=n_results)\n",
    "        progress_bar.set_description('Searching videos')\n",
    "        for search_response in response_list:\n",
    "            if not 'items' in search_response:\n",
    "                continue\n",
    "\n",
    "            for item in search_response['items']:\n",
    "                self._insert_channel(item['snippet']['channelId'])\n",
    "                values = (\n",
    "                    item['id']['videoId'],\n",
    "                    item['snippet']['channelId'],\n",
    "                    str(pd.to_datetime(item['snippet']['publishedAt'])),\n",
    "                    item['snippet']['title'],\n",
    "                    item['snippet']['description']\n",
    "                )\n",
    "                value_list.append(values)\n",
    "                videoId_list.append(item['id']['videoId'])\n",
    "                progress_bar.update(1)\n",
    "        \n",
    "        sql = f'INSERT OR {self.conflict_resolution} INTO videos VALUES(?,?,?,?,?)'\n",
    "        if value_list:\n",
    "            self.cursor.executemany(sql, value_list)\n",
    "        return videoId_list\n",
    "   \n",
    "\n",
    "    def _update_emotions(self):\n",
    "        \"\"\" Update the 5 emotion columns of the 'comments' table\n",
    "        via IBM Watson's Natural Language Understanding API.\n",
    "        \"\"\"\n",
    "        sql_select = \"\"\"\n",
    "            SELECT id, text, language\n",
    "            FROM comments\n",
    "            WHERE anger IS NULL AND text IS NOT NULL\n",
    "        \"\"\"\n",
    "        df = pd.read_sql_query(sql_select, self.conn)\n",
    "        df = df[df['language'].isin(WATSON_SUPPORTED_LANG)]\n",
    "        \n",
    "        if not df.empty:\n",
    "            sql_update = \"\"\"\n",
    "                UPDATE comments\n",
    "                SET anger = ?, disgust = ?, fear = ?, joy = ?, sadness = ?\n",
    "                WHERE id = ?\n",
    "            \"\"\"\n",
    "            values = get_emotions(df, self.watsonApiKey, self.watsonBaseUrl)\n",
    "            if values:\n",
    "                self.cursor.executemany(sql_update, values)\n",
    "                \n",
    "        # Set 'N/A' to comments not supported by the API\n",
    "        df = pd.read_sql_query(sql_select, self.conn)\n",
    "        sql_update = \"\"\"\n",
    "            UPDATE comments\n",
    "            SET anger = 'N/A', disgust = 'N/A', fear = 'N/A', joy = 'N/A', sadness = 'N/A'\n",
    "            WHERE id = ?\n",
    "        \"\"\"\n",
    "        values = [(row.id,) for row in df.itertuples()]\n",
    "        if values:\n",
    "            self.cursor.executemany(sql_update, values)\n",
    "\n",
    "\n",
    "    def _update_keywords(self):\n",
    "        \"\"\" Update the 'keywords' column of the 'comments' table\n",
    "        via MS Azure's Text Analytics API if the detected\n",
    "        language is supported by the API.\n",
    "        \"\"\"\n",
    "        sql_select = \"\"\"\n",
    "            SELECT id, language, text\n",
    "            FROM comments\n",
    "            WHERE keywords IS NULL AND text IS NOT NULL\n",
    "        \"\"\"\n",
    "        df = pd.read_sql_query(sql_select, self.conn)\n",
    "        df = df[df['language'].isin(AZURE_SUPPORTED_LANG)]\n",
    "\n",
    "        if not df.empty:\n",
    "            key_phrases = []\n",
    "            progress_bar = tqdm(total=df.shape[0])\n",
    "            progress_bar.set_description('Updating keywords')\n",
    "            for i in range(0, df.shape[0], AZURE_MAX_DOCUMENTS):\n",
    "                # maximum number of documents in a request: 1000\n",
    "                n_documents = AZURE_MAX_DOCUMENTS if df.shape[0] - i > AZURE_MAX_DOCUMENTS else df.shape[0] - i\n",
    "                documents = {\n",
    "                    'documents': df.iloc[i:i + n_documents].to_dict('records')\n",
    "                }\n",
    "                response = get_key_phrases(documents, self.azureApiKey, self.azureBaseUrl)\n",
    "                if 'documents' in response:\n",
    "                    key_phrases.extend(response['documents'])\n",
    "                # time sleep not to exceed the API requests limit\n",
    "                if i + AZURE_MAX_DOCUMENTS < df.shape[0]:\n",
    "                    sleep(1)\n",
    "                progress_bar.update(n_documents)\n",
    "\n",
    "            sql_update = 'UPDATE comments SET keywords = ? WHERE id = ?'\n",
    "            values = [(','.join(elem['keyPhrases']), elem['id']) for elem in key_phrases]\n",
    "            if values:\n",
    "                self.cursor.executemany(sql_update, values)\n",
    "        \n",
    "        # Set 'N/A' to comments not supported by the API\n",
    "        df = pd.read_sql_query(sql_select, self.conn)\n",
    "        sql_update = \"UPDATE comments SET keywords = 'N/A' WHERE id = ?\"\n",
    "        values = [(row.id,) for row in df.itertuples()]\n",
    "        if values:\n",
    "            self.cursor.executemany(sql_update, values)\n",
    "\n",
    "\n",
    "    def _update_languages(self):\n",
    "        \"\"\" Update the 'language' column of the 'comments' table.\n",
    "        \"\"\"        \n",
    "        sql_select = 'SELECT id, text FROM comments WHERE language IS NULL AND text IS NOT NULL'\n",
    "        df = pd.read_sql_query(sql_select, self.conn)\n",
    "        if df.empty:\n",
    "            return\n",
    "        \n",
    "        if not df.empty:\n",
    "            sentiments = []\n",
    "            progress_bar = tqdm(total=df.shape[0])\n",
    "            progress_bar.set_description('Updating languages')\n",
    "            for i in range(0, df.shape[0], AZURE_MAX_DOCUMENTS):\n",
    "                # maximum number of documents in a request: 1000\n",
    "                n_documents = AZURE_MAX_DOCUMENTS if df.shape[0] - i > AZURE_MAX_DOCUMENTS else df.shape[0] - i\n",
    "                documents = {\n",
    "                    'documents': df.iloc[i:i + n_documents].to_dict('records')\n",
    "                }\n",
    "                response = get_languages(documents, self.azureApiKey, self.azureBaseUrl)\n",
    "                if 'documents' in response:\n",
    "                    sentiments.extend(response['documents'])\n",
    "                # time sleep not to exceed the API requests limit\n",
    "                if i + AZURE_MAX_DOCUMENTS < df.shape[0]:\n",
    "                    sleep(1)\n",
    "                progress_bar.update(n_documents)\n",
    "            \n",
    "            sql_update = 'UPDATE comments SET language = ? WHERE id = ?'\n",
    "            values = [(elem['detectedLanguages'][0]['iso6391Name'], elem['id'])\n",
    "                      for elem in sentiments]\n",
    "            if values:\n",
    "                self.cursor.executemany(sql_update, values)\n",
    "\n",
    "        # Set '(Unknown)' to comments not supported by the API\n",
    "        df = pd.read_sql_query(sql_select, self.conn)\n",
    "        sql_update = \"UPDATE comments SET language = '(Unknown)' WHERE id = ?\"\n",
    "        values = [(row.id,) for row in df.itertuples()]\n",
    "        if values:\n",
    "            self.cursor.executemany(sql_update, values)\n",
    "        \n",
    "\n",
    "    def _update_sentiments(self):\n",
    "        \"\"\" Update the 'sentimentScore' and 'sentimentLabel' columns of the 'comments' table\n",
    "        via MS Azure's Text Analytics API.\n",
    "        \"\"\"\n",
    "        sql_select = \"\"\"\n",
    "            SELECT id, language, text\n",
    "            FROM comments\n",
    "            WHERE sentimentScore IS NULL AND text IS NOT NULL\n",
    "        \"\"\"\n",
    "        df = pd.read_sql_query(sql_select, self.conn)\n",
    "        df = df[df['language'].isin(AZURE_SUPPORTED_LANG)]\n",
    "\n",
    "        if not df.empty:\n",
    "            sentiments = []\n",
    "            progress_bar = tqdm(total=df.shape[0])\n",
    "            progress_bar.set_description('Updating sentiments')\n",
    "            for i in range(0, df.shape[0], AZURE_MAX_DOCUMENTS):\n",
    "                # maximum number of documents in a request: 1000\n",
    "                n_documents = AZURE_MAX_DOCUMENTS if df.shape[0] - i > AZURE_MAX_DOCUMENTS else df.shape[0] - i\n",
    "                documents = {\n",
    "                    'documents': df.iloc[i:i + n_documents].to_dict('records')\n",
    "                }\n",
    "                response = get_sentiments(documents, self.azureApiKey, self.azureBaseUrl)\n",
    "                if 'documents' in response:\n",
    "                    sentiments.extend(response['documents'])\n",
    "                # time sleep not to exceed the API requests limit\n",
    "                if i + AZURE_MAX_DOCUMENTS < df.shape[0]:\n",
    "                    sleep(1)\n",
    "                progress_bar.update(n_documents)\n",
    "\n",
    "            for elem in sentiments:\n",
    "                if elem['score'] < 0.4:\n",
    "                    elem['label'] = 'negative'\n",
    "                elif elem['score'] < 0.7:\n",
    "                    elem['label'] = 'neutral'\n",
    "                else:\n",
    "                    elem['label'] = 'positive'\n",
    "\n",
    "            sql_update = \"\"\"\n",
    "                UPDATE comments\n",
    "                SET sentimentLabel = ?, sentimentScore = ?\n",
    "                WHERE id = ?\n",
    "            \"\"\"\n",
    "            values = [(elem['label'], elem['score'], elem['id']) for elem in sentiments]\n",
    "            if values:\n",
    "                self.cursor.executemany(sql_update, values)\n",
    "        \n",
    "        # Set 'N/A' to comments not supported by the API\n",
    "        df = pd.read_sql_query(sql_select, self.conn)\n",
    "        sql_update = \"\"\"\n",
    "            UPDATE comments\n",
    "            SET sentimentLabel = 'N/A', sentimentScore = 'N/A'\n",
    "            WHERE id = ?\n",
    "        \"\"\"\n",
    "        values = [(row.id,) for row in df.itertuples()]\n",
    "        if values:\n",
    "            self.cursor.executemany(sql_update, values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'db' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-168085b004b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'db' is not defined"
     ]
    }
   ],
   "source": [
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** YouTube database directory: /home/kadogams/dev/youtube/youtube_analyzer/data/youtube.sqlite *****\n"
     ]
    }
   ],
   "source": [
    "with open('MY_CREDENTIALS.json', 'r') as f:\n",
    "    credentials = json.load(f)\n",
    "\n",
    "db = youtubeAnalyzer(\n",
    "    googleApiKey=credentials['google_developer_key'],\n",
    "    azureApiKey=credentials['azure_subscription_key'],\n",
    "    azureBaseUrl=credentials['azure_text_analytics_base_url'],\n",
    "    watsonApiKey=credentials['watson_nlu_api_key'],\n",
    "    watsonBaseUrl=credentials['watson_nlu_base_url']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE channels (\n",
      "    id TEXT PRIMARY KEY NOT NULL,\n",
      "    title TEXT,\n",
      "    description TEXT,\n",
      "    country TEXT\n",
      ") \n",
      "\n",
      "CREATE TABLE comments (\n",
      "    id TEXT PRIMARY KEY NOT NULL,\n",
      "    videoId TEXT NOT NULL,\n",
      "    authorChannelId TEXT,\n",
      "    publishedAt TIMESTAMP NOT NULL,\n",
      "    likeCount INTEGER NOT NULL,\n",
      "    parentId TEXT,\n",
      "    text TEXT,\n",
      "    language TEXT,\n",
      "    keywords TEXT,\n",
      "    sentimentLabel TEXT,\n",
      "    sentimentScore REAL,\n",
      "    anger REAL,\n",
      "    disgust REAL,\n",
      "    fear REAL,\n",
      "    joy REAL,\n",
      "    sadness REAL\n",
      ") \n",
      "\n",
      "CREATE TABLE videos (\n",
      "    id TEXT PRIMARY KEY NOT NULL,\n",
      "    channelId TEXT,\n",
      "    publishedAt TIMESTAMP NOT NULL,\n",
      "    title TEXT,\n",
      "    description TEXT\n",
      ") \n",
      "\n"
     ]
    }
   ],
   "source": [
    "db.create_structure()\n",
    "db.display_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# db.query('DROP TABLE videos')\n",
    "# db.conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 234/1650 [01:21<07:15,  3.25it/s]"
     ]
    },
    {
     "ename": "ApiException",
     "evalue": "Error: not enough text for language id, Code: 422 , X-global-transaction-id: 8f23e07c19cedb761c1a527a20251b09",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------\u001b[0m",
      "\u001b[0;31mApiException\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-1efeed5a31ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# db._update_sentiments()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# db._update_keywords()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_emotions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-1d24fa843242>\u001b[0m in \u001b[0;36m_update_emotions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    474\u001b[0m                 \u001b[0mWHERE\u001b[0m \u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m             \"\"\"\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_emotions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatsonApiKey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatsonBaseUrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutemany\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql_update\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/youtube/youtube_analyzer/watson_api.py\u001b[0m in \u001b[0;36mget_emotions\u001b[0;34m(df, api_key, nlu_base_url)\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 features=Features(\n\u001b[0;32m---> 72\u001b[0;31m                     \u001b[0memotion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEmotionOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m                 )\n\u001b[1;32m     74\u001b[0m             ).get_result()\n",
      "\u001b[0;32m~/anaconda3/envs/wap/lib/python3.7/site-packages/ibm_watson/natural_language_understanding_v1.py\u001b[0m in \u001b[0;36manalyze\u001b[0;34m(self, features, text, html, url, clean, xpath, fallback_to_raw, return_analyzed_text, language, limit_text_characters, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m             accept_json=True)\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/wap/lib/python3.7/site-packages/ibm_cloud_sdk_core/base_service.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, accept_json, headers, params, json, data, files, **kwargs)\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0merror_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Unauthorized: Access is denied due to '\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m                                 \u001b[0;34m'invalid credentials'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mApiException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_response\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mApiException\u001b[0m: Error: not enough text for language id, Code: 422 , X-global-transaction-id: 8f23e07c19cedb761c1a527a20251b09"
     ]
    }
   ],
   "source": [
    "# db._update_languages()\n",
    "# db._update_sentiments()\n",
    "# db._update_keywords()\n",
    "db._update_emotions()\n",
    "db.conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbf786f655114ea0ae5bf963f8a8d1bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a76f58decd4b1ba68fa0457557379c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de0fb8ed8f6b49c083c06d447027c8c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a602f0973b4510a0f8a2b11b5beafb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "Search results stored in '/home/kadogams/dev/youtube/youtube_analyzer/data/youtube.sqlite'\n"
     ]
    }
   ],
   "source": [
    "db.search('brexit', n_results=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('PvoUIKelXK0',\n",
       "  'UC16niRr50-MSBwiO3YDb3RA',\n",
       "  '2019-07-29 21:45:34+00:00',\n",
       "  'Brexit: UK’s new PM accused of pursuing ‘no-deal’ - BBC News',\n",
       "  \"Britain's new Prime Minister Boris Johnson says he's 'very confident' a new Brexit deal can be reached with the European Union. Speaking on his first visit to ...\"),\n",
       " ('BQHoDCNEws0',\n",
       "  'UCSMqateX8OA2s1wsOR2EgJA',\n",
       "  '2019-07-30 08:27:24+00:00',\n",
       "  'What Deal Does the EU Want From Brexit? - Brexit Explained',\n",
       "  \"There's been a lot of talk, on this channel and elsewhere, about what kind of Brexit deal the UK wants. There's been significantly less conversation about what ...\"),\n",
       " ('L6O9U4NB1_g',\n",
       "  'UC6o-wWU-v2ClFMwougmK7dA',\n",
       "  '2019-07-30 10:36:23+00:00',\n",
       "  'How could &#39;no-deal Brexit&#39; impact the union? – BBC Newsnight',\n",
       "  'Is a no-deal Brexit still a one in a million chance or is it starting to look like the most probable outcome? Subscribe to our channel here: https://goo.gl/31Q53F ...'),\n",
       " ('q4LR9j44ra0',\n",
       "  'UCIzXayRP7-P0ANpq-nD-h5g',\n",
       "  '2019-07-30 15:18:35+00:00',\n",
       "  'PM Boris: &#39;We&#39;re not aiming for a no deal Brexit&#39;',\n",
       "  \"BORIS Johnson has promised Welsh farmers bumper trade deals around the world after Brexit to sell their products - but they've threatened to riot if there's a No ...\"),\n",
       " ('_MZL4UYb0YY',\n",
       "  'UCTrQ7HXWRRxr7OsOtodr2_w',\n",
       "  '2019-07-29 18:29:42+00:00',\n",
       "  'Boris Johnson in Scotland as pound falls amid No Deal Brexit fears',\n",
       "  'I want us to “go the extra thousand miles” to reach a new Brexit deal, Boris Johnson declared today, describing the existing agreement as “dead”. Subscribe: ...'),\n",
       " ('Zb0MI0vKlSo',\n",
       "  'UCE3181Md63Eg7uTybf6HgNw',\n",
       "  '2019-07-30 09:38:36+00:00',\n",
       "  'Nigel Farage Attacks the Boris Johnson Brexit Plan!',\n",
       "  'Nigel Farage has fired a warning shot at the new Prime Minister, Boris Johnson, by attacking his Brexit plan. PLEASE SUPPORT MY YOUTUBE WORK ON ...'),\n",
       " ('aJdVG6x1X7U',\n",
       "  'UCIzXayRP7-P0ANpq-nD-h5g',\n",
       "  '2019-07-29 13:32:54+00:00',\n",
       "  'Boris Johnson aiming for a new deal over no deal Brexit',\n",
       "  'Faslane, Scotland. Boris Johnson said there was every chance of striking a new Brexit deal with the European Union and that he wanted a grand new trade deal ...'),\n",
       " ('jLLBF4iDOTk',\n",
       "  'UCSMqateX8OA2s1wsOR2EgJA',\n",
       "  '2019-07-28 08:30:01+00:00',\n",
       "  'What The EU Thinks of Johnson&#39;s Brexit Plan - Brexit Explained',\n",
       "  \"Johnson's Brexit Plan: https://youtu.be/yiKswD3rrkQ Does the UK Need to Pay £39 Billion: https://youtu.be/38jyH9CJ8ls May's Last Speech to the HoC: ...\"),\n",
       " ('iP3SaJhkvQc',\n",
       "  'UCZO5CoTyLqv72wGluKwBzrA',\n",
       "  '2019-07-29 18:23:59+00:00',\n",
       "  'EU pours cold water over UK Brexit fake news: EU ‘protected’ from no-deal Brexit, Brussels insists',\n",
       "  \"Brussels has hit back at claims the EU is unprepared to cope with the fallout of a no-deal Brexit, rejecting a report from Britain's largest employer organisation ...\"),\n",
       " ('T-k_acxPlIU',\n",
       "  'UCTrQ7HXWRRxr7OsOtodr2_w',\n",
       "  '2019-07-26 14:10:00+00:00',\n",
       "  'No-deal? Extension? Election? - Boris Johnson&#39;s Brexit plan',\n",
       "  'As Boris Johnson moves into number 10, Francis Elliott (Political Editor, The Times) and Peter Foster (Europe Editor, The Daily Telegraph) thrash out the ...'),\n",
       " ('RdpaMuqk2sc',\n",
       "  'UCQfwfsi5VrQ8yKZ-UWmAEFg',\n",
       "  '2019-07-08 13:15:47+00:00',\n",
       "  'John Bercow on Brexit: &#39;A second referendum is possible&#39;',\n",
       "  'Subscribe to France 24 now: http://f24.my/youtubeEN FRANCE 24 live news stream: all the latest news 24/7 http://f24.my/YTliveEN In an interview with FRANCE ...'),\n",
       " ('bCiqKV4_FDw',\n",
       "  'UCZO5CoTyLqv72wGluKwBzrA',\n",
       "  '2019-07-25 18:45:41+00:00',\n",
       "  'BoJo warned: EU will not renegotiate Brexit deal, Juncker tells Johnson (25th July 2019)',\n",
       "  'The European commission president, Jean-Claude Juncker, has told Boris Johnson that the EU27 will not give in to his demand to renegotiate the Brexit ...'),\n",
       " ('SlVO2Rcix_g',\n",
       "  'UCoMdktPbSTixAyNGwb-UYkQ',\n",
       "  '2019-07-28 09:57:07+00:00',\n",
       "  'Swinson: &#39;There&#39;s not a big enough majority in this country for Brexit&#39;',\n",
       "  \"The new Liberal Democrat leader Jo Swinson tells Sophy Ridge there's not a big enough majority in this country for Brexit - and we need to put thE issue back to ...\"),\n",
       " ('8tF_n0abgVY',\n",
       "  'UCSMqateX8OA2s1wsOR2EgJA',\n",
       "  '2019-07-22 08:15:02+00:00',\n",
       "  'What No Deal Brexit Will Do To The Economy - Brexit Explained',\n",
       "  \"Support TLDR on Patreon: http://www.patreon.com/tldrnews Johnson's Brexit Plan Explained: https://youtu.be/yiKswD3rrkQ For a long time, there's been talk ...\"),\n",
       " ('3vydV-3KQJQ',\n",
       "  'UC8p1vwvWtl6T73JiExfWs1g',\n",
       "  '2019-07-26 16:22:23+00:00',\n",
       "  'EU rejects Boris Johnson&#39;s push for a new Brexit deal',\n",
       "  \"The UK's Brexit crisis is escalating as the European Union rejects Prime Minister Boris Johnson's push for a new deal. He wants to redo Theresa May's ...\"),\n",
       " ('e2pdH9FnGrs',\n",
       "  'UCupvZG-5ko_eiXAupbDfxWw',\n",
       "  '2019-03-27 22:38:54+00:00',\n",
       "  'Watch chaos in Parliament after Brexit votes fail',\n",
       "  'None of the eight options that British lawmakers voted on previously got majority support from MPs. Conservative lawmaker Oliver Letwin, the architect of the ...'),\n",
       " ('Aos8nYMG20Y',\n",
       "  'UCE3181Md63Eg7uTybf6HgNw',\n",
       "  '2019-07-29 14:57:30+00:00',\n",
       "  'Ramping up for a no deal Brexit!',\n",
       "  'Under the new PM, Boris Johnson, the government is seriously ramping up no deal Brexit preparations. PLEASE SUPPORT MY YOUTUBE WORK ON ...'),\n",
       " ('agZ0xISi40E',\n",
       "  'UC2C_jShtL725hvbm1arSV9w',\n",
       "  '2019-03-10 15:05:13+00:00',\n",
       "  '* The EU&#39;s &#39;SECRET&#39; Brexit Negotiation EXPOSED 🙄',\n",
       "  'Main video: https://www.youtube.com/watch?v=J1Yv24cM2os&list=PLqs5ohhass_QREwr7tdW8gh3kLgAwX8wh&index=2.'),\n",
       " ('-ecq9vTg_mU',\n",
       "  'UCcR1wBpaUWcmsAvmE-nCRMQ',\n",
       "  '2019-07-27 17:50:49+00:00',\n",
       "  'Brexit Shambles: Johnson&#39;s &#39;golden age&#39; opens with a rebuff from the EU',\n",
       "  'BBC News at Six, Channel 4 News, ITV News at Ten & Newsnight 25 July 2019 IMPORTANT NOTICE July 2019: This channel will be going on holiday shortly, ...'),\n",
       " ('krF6Hsin2rM',\n",
       "  'UCIALMKvObZNtJ6AmdCLP7Lg',\n",
       "  '2019-07-29 07:14:52+00:00',\n",
       "  'Markets Divided on No-Deal Brexit',\n",
       "  'Jul.29 -- U.K. Prime Minister Boris Johnson and his Brexit cabinet hold their first meeting today. It will gather every day to make sure the country leaves on ...')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = \"SELECT * FROM videos\" \n",
    "db.query(sql)\n",
    "tmp = db.cursor.fetchall()\n",
    "len(tmp)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1674, 17)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = db.get_comments_df(\n",
    "#     video_search='chocolate, cream'\n",
    ")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 17)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[~df['anger'].isna()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
